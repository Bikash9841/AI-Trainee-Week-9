{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdnCGI5e1lOl"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOrp7Bms1dky"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Huggingface datasets and tokenizers\n",
        "# from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kNge-Lr37SB"
      },
      "source": [
        "## Input and Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJjtlSRJ1pV5"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int=512, vocab_size: int=1000) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int=512, seq_len: int=200, dropout: float=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position = torch.arange(\n",
        "            0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float(\n",
        "        ) * (-math.log(10000.0) / d_model))  # (d_model / 2)\n",
        "        # Apply sine to even indices\n",
        "        # sin(position * (10000 ** (2i / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices\n",
        "        # cos(position * (10000 ** (2i / d_model))\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # Add a batch dimension to the positional encoding\n",
        "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
        "        # Register the positional encoding as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model)\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9woq94FA186K",
        "outputId": "adaf5602-2a57-4d5a-a486-0f0228549082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the shape of our example input is: torch.Size([5, 12])\n"
          ]
        }
      ],
      "source": [
        "example=torch.randint(0,999,(5,12))\n",
        "print(f\"the shape of our example input is: {example.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX7YSDJG2UMy",
        "outputId": "47d2a03a-2f5e-471f-9687-fd337d3979e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after generating input Embedding: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "inp_e=InputEmbeddings(512,1000)\n",
        "print(f\"Shape after generating input Embedding: {inp_e(example).shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz0W82L52aDQ",
        "outputId": "9b611c45-46ee-4390-cfed-15fe43a64017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after adding positional embedding: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "pos_e=PositionalEncoding(512,200,0.1)\n",
        "pos_e_out=pos_e(inp_e(example))\n",
        "print(f\"Shape after adding positional embedding: {pos_e_out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKv7D2aL326C"
      },
      "source": [
        "## Multihead Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQMvo3Ui2z9r"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int=512, h: int=8, dropout: float=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # Embedding vector size\n",
        "        self.h = h  # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h  # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)  # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)  # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        query = self.w_q(q)\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k)\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(\n",
        "            query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1],\n",
        "                       self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(\n",
        "            value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(\n",
        "            query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(\n",
        "            x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSKlTxfg4AnB",
        "outputId": "1e55443b-8b08-4e65-a556-e0708ac3443c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after MHSA Layer: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "# MHSA Layer\n",
        "mhsa=MultiHeadAttentionBlock()\n",
        "mhsa_out=mhsa(pos_e_out,pos_e_out,pos_e_out,None)\n",
        "print(f\"Shape after MHSA Layer: {mhsa_out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDYXbJtmZjtK"
      },
      "source": [
        "## Layer Normalization and FeedForward Block and Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udco4VlHZd1D"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int=0, eps: float = 10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # alpha is a learnable parameter\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        # bias is a learnable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "        # Keep the dimension for broadcasting\n",
        "        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
        "        # Keep the dimension for broadcasting\n",
        "        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
        "        # eps is to prevent dividing by zero or when std is very small\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)  # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)  # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    # many transformer implmentation also do like this--> normalize the input + positional embedding, then apply mhsa and add skip connection.\n",
        "    # here sublayer is MHSA\n",
        "    def forward(self, x, sublayer):\n",
        "      return x + self.dropout(self.norm(sublayer(x)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5B9rxG3Ztzy",
        "outputId": "77b1f530-cbac-488a-bd78-f4f1204e1917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after AddNorm Layer: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "# residual connection\n",
        "res_con=ResidualConnection(0,0.1)\n",
        "add_norm_out=res_con(pos_e_out,lambda x: mhsa(x, x, x, None))\n",
        "\n",
        "print(f\"Shape after AddNorm Layer: {add_norm_out.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feed forward layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVWdk4Bkq6U1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
