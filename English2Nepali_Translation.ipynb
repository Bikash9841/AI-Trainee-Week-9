{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6UaEzYCZR1N",
        "outputId": "fe371e27-27c9-43f5-8759-27b78553f2fc"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WgmsbvsXmvg2"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n0Er0SRNtE78"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdnCGI5e1lOl"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oOrp7Bms1dky"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bikasherl/miniconda3/envs/bajra/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Huggingface datasets and tokenizers\n",
        "# from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from datasets import load_dataset\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kNge-Lr37SB"
      },
      "source": [
        "## Input and Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xJjtlSRJ1pV5"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int=512, vocab_size: int=1000) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int=512, seq_len: int=200, dropout: float=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position = torch.arange(\n",
        "            0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float(\n",
        "        ) * (-math.log(10000.0) / d_model))  # (d_model / 2)\n",
        "        # Apply sine to even indices\n",
        "        # sin(position * (10000 ** (2i / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices\n",
        "        # cos(position * (10000 ** (2i / d_model))\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # Add a batch dimension to the positional encoding\n",
        "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
        "        # Register the positional encoding as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model)\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9woq94FA186K",
        "outputId": "279ed0ad-058d-41bd-9fc8-f535285d4650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the shape of our example input is: torch.Size([5, 12])\n"
          ]
        }
      ],
      "source": [
        "example=torch.randint(0,999,(5,12))\n",
        "print(f\"the shape of our example input is: {example.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX7YSDJG2UMy",
        "outputId": "f41ad523-3e0f-4cfe-c2a5-34023166d299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after generating input Embedding: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "inp_e=InputEmbeddings(512,1000)\n",
        "print(f\"Shape after generating input Embedding: {inp_e(example).shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz0W82L52aDQ",
        "outputId": "4214ef81-31de-496b-c654-bc03969808d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after adding positional embedding: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "pos_e=PositionalEncoding(512,200,0.1)\n",
        "pos_e_out=pos_e(inp_e(example))\n",
        "print(f\"Shape after adding positional embedding: {pos_e_out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKv7D2aL326C"
      },
      "source": [
        "## Multihead Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PQMvo3Ui2z9r"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int=512, h: int=8, dropout: float=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # Embedding vector size\n",
        "        self.h = h  # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h  # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)  # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)  # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        query = self.w_q(q)\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k)\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(\n",
        "            query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1],\n",
        "                       self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(\n",
        "            value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(\n",
        "            query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(\n",
        "            x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSKlTxfg4AnB",
        "outputId": "8b20f99d-3eea-4519-8854-9a9c6368b734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after MHSA Layer: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "# MHSA Layer\n",
        "mhsa=MultiHeadAttentionBlock()\n",
        "mhsa_out=mhsa(pos_e_out,pos_e_out,pos_e_out,None)\n",
        "print(f\"Shape after MHSA Layer: {mhsa_out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDYXbJtmZjtK"
      },
      "source": [
        "## Layer Normalization and FeedForward Block and Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Udco4VlHZd1D"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, eps: float = 10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # alpha is a learnable parameter\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        # bias is a learnable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "        # Keep the dimension for broadcasting\n",
        "        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
        "        # Keep the dimension for broadcasting\n",
        "        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
        "        # eps is to prevent dividing by zero or when std is very small\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)  # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)  # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    # many transformer implmentation also do like this--> normalize the input + positional embedding, then apply mhsa and add skip connection.\n",
        "    # here sublayer is MHSA\n",
        "    def forward(self, x, sublayer):\n",
        "      return x + self.dropout(self.norm(sublayer(x)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5B9rxG3Ztzy",
        "outputId": "0a741ac4-4ea0-4392-b6d2-58a3ec60afba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after AddNorm Layer: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "# residual connection\n",
        "res_con=ResidualConnection(features=512,dropout=0.1)\n",
        "add_norm_out=res_con(pos_e_out,lambda x: mhsa(x, x, x, None))\n",
        "\n",
        "print(f\"Shape after AddNorm Layer: {add_norm_out.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVWdk4Bkq6U1",
        "outputId": "62d1b9a3-9036-4924-ac1a-2840bbf3feae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after FeedForward Layer: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "# feedforward\n",
        "\n",
        "ff=FeedForwardBlock(d_model=512,d_ff=2048,dropout=0.1)\n",
        "ff_out=ff(add_norm_out)\n",
        "\n",
        "print(f\"Shape after FeedForward Layer: {ff_out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDsEWfTo8hj0"
      },
      "source": [
        "## Building the Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ac_tbcqp8acF"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList(\n",
        "            [ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](\n",
        "            x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ecHs_js8njE",
        "outputId": "aa88a3dc-c41a-4110-be4b-b19ad3efe748"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 12, 512])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a single encoder block\n",
        "encoder_block=EncoderBlock(features=512,self_attention_block=mhsa,feed_forward_block=ff,dropout=0.1)\n",
        "encoder_block_out=encoder_block(pos_e_out,src_mask=None)\n",
        "encoder_block_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AwCNIPo0BjPt"
      },
      "outputs": [],
      "source": [
        "# Six stacks of encoder block made up one Encoder of Transformer\n",
        "\n",
        "encoder_blocks=[encoder_block,encoder_block,encoder_block,encoder_block,encoder_block,encoder_block]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDC4K98dAGLK",
        "outputId": "07382166-ac0c-4d0c-855d-cc1fd4646d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The final output of encoder block is: torch.Size([5, 12, 512])\n"
          ]
        }
      ],
      "source": [
        "# Complete Encoder Block\n",
        "encoder = Encoder(512, nn.ModuleList(encoder_blocks))\n",
        "encoder_out=encoder(pos_e_out,None)\n",
        "\n",
        "\n",
        "print(f\"The final output of encoder block is: {encoder_out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CxYNvyMD4xM"
      },
      "source": [
        "## Building the Decoder Architecture along with Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ObiRtww-EU5B"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList(\n",
        "            [ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](\n",
        "            x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(\n",
        "            x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cikhFZf9EYo1"
      },
      "source": [
        "## Building the <b>Transformer Architecture</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a0nB2HWyElqu"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(\n",
        "            d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(\n",
        "            d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(\n",
        "            d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(\n",
        "            d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block,\n",
        "                                     decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(\n",
        "        encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR2DKcwpiisr"
      },
      "source": [
        "## Building the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P3YfdodpiXUA"
      },
      "outputs": [],
      "source": [
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor(\n",
        "            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor(\n",
        "            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor(\n",
        "            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_text = src_target_pair[self.src_lang]\n",
        "        tgt_text = src_target_pair[self.tgt_lang]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.seq_len - \\\n",
        "            len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] *\n",
        "                             enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] *\n",
        "                             dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] *\n",
        "                             dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all seq_len long\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (seq_len)\n",
        "            \"decoder_input\": decoder_input,  # (seq_len)\n",
        "            # (1, 1, seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "            # (1, seq_len) & (1, seq_len, seq_len),\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
        "            \"label\": label,  # (seq_len)\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LVoI8rYMonKI"
      },
      "outputs": [],
      "source": [
        "def get_ds(config):\n",
        "    # It only has the train split, so we divide it overselves\n",
        "    ds_raw = load_dataset(f\"{config['datasource']}\",split='train[:200]')\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Keep 90% for training, 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(\n",
        "        ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt,\n",
        "                                config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt,\n",
        "                              config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(\n",
        "            item[config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(\n",
        "            item[config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF0M7kTRiw9w"
      },
      "source": [
        "## Building Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7IqARZq9itTo"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 12,\n",
        "        \"num_epochs\": 600,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 110,\n",
        "        \"d_model\": 512,\n",
        "        \"datasource\": 'CohleM/english-to-nepali',\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"ne\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": \"latest\",\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"/content/drive/MyDrive/English2Nepali/runs\"\n",
        "        # \"experiment_name\": \"/content/drive/MyDrive/translation/runs\"\n",
        "    }\n",
        "\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "    # return str(Path('/content/drive/MyDrive/') / model_folder / model_filename)\n",
        "    return str(Path('/content/drive/MyDrive/English2Nepali/') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = f\"/content/drive/MyDrive/English2Nepali/{config['datasource']}_{config['model_folder']}\"\n",
        "    # model_folder = f\"/content/drive/MyDrive/{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn3IBDSpmUMq"
      },
      "source": [
        "## Making Separate Tokenizer for English and Nepali Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0DOBbS9Bma5o"
      },
      "outputs": [],
      "source": [
        "#loading english to nepali dataset\n",
        "# ds=load_dataset('CohleM/english-to-nepali')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WlHBg6bPmCJk"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences(ds,lang):\n",
        "    for item in ds[lang]:\n",
        "        yield item\n",
        "\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    # tokenizer_path = Path(\"/content/drive/MyDrive/English2Nepali\"+config['tokenizer_file'].format(lang))\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(\n",
        "            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(\n",
        "            get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ztremKXpm6Hb"
      },
      "outputs": [],
      "source": [
        "# making the tokenizer and saving it into the drive\n",
        "# config=get_config()\n",
        "\n",
        "# tok=get_or_build_tokenizer(config,ds,'ne')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOAqcAuxsnT8"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xibyL_ZKoj4g",
        "outputId": "9c17f2bf-cacb-4dbd-e6fd-eee0ffce74f0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(\n",
        "            1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, source_mask,\n",
        "                           decoder_input, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device)  # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(\n",
        "                device)  # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(\n",
        "                model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(\n",
        "                model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            # print(count)\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "    if writer:\n",
        "        # Evaluate the character error rate\n",
        "        # Compute the char error rate\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the word error rate\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the BLEU metric\n",
        "        metric = torchmetrics.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len,\n",
        "                              config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(config):\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available(\n",
        "    ) else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "\n",
        "    else:\n",
        "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
        "\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # Make sure the weights folder exists\n",
        "    Path(f\"/content/drive/MyDrive/English2Nepali/{config['datasource']}_{config['model_folder']}\").mkdir(\n",
        "        parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(\n",
        "        config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(),\n",
        "                      tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    # If the user specified a model to preload before training, load it\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    preload = config['preload']\n",
        "    model_filename = latest_weights_file_path(\n",
        "        config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\n",
        "        '[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        # torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(\n",
        "            train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device)  # (b, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device)  # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(\n",
        "                device)  # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(\n",
        "                device)  # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)  # (B, seq_len, d_model)\n",
        "            decoder_output = model.decode(\n",
        "                encoder_output, encoder_mask, decoder_input, decoder_mask)  # (B, seq_len, d_model)\n",
        "            # (B, seq_len, vocab_size)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            # Compare the output with the label\n",
        "            label = batch['label'].to(device)  # (B, seq_len)\n",
        "\n",
        "            # Compute the loss using a simple cross entropy\n",
        "            loss = loss_fn(\n",
        "                proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # Log the loss\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Run validation at the end of every epoch\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt,\n",
        "                       config['seq_len'], device, lambda msg: batch_iterator.write(msg), epoch, writer)\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        if (epoch+1) % 10==0:\n",
        "          torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'global_step': global_step\n",
        "          }, model_filename)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()\n",
        "    # train_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inferencing on Custom Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iWe4Etd_ve7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of source sentence: 99\n",
            "Max length of target sentence: 74\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "config = get_config()\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "# Load the pretrained weights\n",
        "# model_filename = latest_weights_file_path(config)\n",
        "state = torch.load('/media/bikasherl/New Volume/Bajra_Tech/Week 9/tmodel_559.pt')\n",
        "model.load_state_dict(state['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "stty: 'standard input': Inappropriate ioctl for device\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Requiring declaration of notifiable diseases.\n",
            "    TARGET: (३) सूचना गर्नुपर्ने रोगलाई प्रकाश गर्नु नै पर्ने कानून\n",
            " PREDICTED: ( ३ ) सूचना गर्नुपर्ने प्रकाश गर्नु नै पर्ने कानून\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Subject to through going study and analysis before decisions are reached, priority consideration will be given to industries for the production of cement, forest products, sugar, textiles, cigarettes, and iron.\n",
            "    TARGET: हाल सरकारले आर्थिक र शिल्पज्ञानकोपूरा अध्ययन गरी उचित ठहरेमा सिमेन्ट, चीनी, चुरोट, कपडा, फलाम तथा बनेको पैदावारसम्बन्धी उद्योगलाई प्राथमिकता दिने ठहर गरेको छ\n",
            " PREDICTED: हाल सरकारले आर्थिक र अध्ययन गरी उचित ठहरेमा सिमेन्ट , , चुरोट , कपडा , फलाम तथा बनेको उद्योगलाई प्राथमिकता दिने ठहर गरेको छ\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Under the Five-Year Plan, work by the Local Health Services will become the responsibility of the Medical Officer through Health specialists, health assistants and female auxiliary health workers, public health nurses, and district health officers.\n",
            "    TARGET: पञ्चबर्िर्षय योजनाअन्तर्गत स्थानीय स्वास्थ्य सेवाको संचालन, स्वास्थ्य विशेषज्ञ, स्वास्थ्य सहायक, नारी उपकारी कार्यकर्ता जनस्वास्थ्य परिचारिका, जिल्ला स्वास्थ्य अधिकारीको सहायता लिई एक मेडिकल अफिसरले गर्नेछ\n",
            " PREDICTED: योजनाअन्तर्गत स्थानीय स्वास्थ्य सेवाको संचालन , स्वास्थ्य विशेषज्ञ , स्वास्थ्य सहायक , नारी कार्यकर्ता जनस्वास्थ्य परिचारिका , जिल्ला स्वास्थ्य अधिकारीको सहायता लिई एक मेडिकल गर्नेछ\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Serious efforts to plan economic development took place after the establishment of a Planning Board in 1955.\n",
            "    TARGET: २०१३ सालमा योजना मण्डलको गठन भएपछि मात्र नेपालमा सँगठित तथा योजनाबद्ध रुपमा आर्थिक विकास गर्ने प्रयास सुरू भयो\n",
            " PREDICTED: २०१३ सालमा योजना मण्डलको गठन भएपछि मात्र नेपालमा सँगठित तथा योजनाबद्ध रुपमा आर्थिक विकास गर्ने प्रयास सुरू भयो\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The Agricultural Reorganization and the Cooperative Bank Acts of 1962 were steps in this direction. The Agricultural Reorganization Act was implemented in selected gram panchyats of Jhapa, Palpa and Chitwan dis tricts.\n",
            "    TARGET: फलतः कृषि पुनर्गठन कार्यक्रमअन्तर्गत झापा, पाल्पा र चितौनका केही पञ्चायतहरूमा कित्ता सर्वे गरी पुरानो परम्पराको भूमि प्रशासनमा परिवर्तन ल्याई भूमि माथि वास्तविक कृषक पत्ता लगाई सम्बन्धित तथ्या र प्रशासन सम्बन्धी आधुनिकीकरण गर्ने काम भयो\n",
            " PREDICTED: फलतः कृषि पुनर्गठन कार्यक्रमअन्तर्गत झापा , पाल्पा र केही कित्ता सर्वे गरी पुरानो परम्पराको भूमि प्रशासनमा परिवर्तन ल्याई भूमि माथि वास्तविक कृषक पत्ता लगाई सम्बन्धित तथ्या र प्रशासन सम्बन्धी आधुनिकीकरण गर्ने काम भयो\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The planned outlay in the public sector was virtually achieved.\n",
            "    TARGET: तालिका नं. १(क) मा विभिन्न क्षेत्रको लागि योजनामा छुट याइएको रकम र   वास्तविक खर्च साथ देखाइएको छ।\n",
            " PREDICTED: तालिका नं . १ ( क ) मा विभिन्न क्षेत्रको लागि योजनामा छुट याइएको रकम र वास्तविक खर्च साथ देखाइएको छ ।\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Ad foreign trade relations - with India, China, and many other nations with whom Nepal has diplomatic relations have not had the desirable foundation of a series of well-drawn treaties of commerce and friendship.\n",
            "    TARGET: सरकारको स्पष्ट वाणिज्य नीति पनि निर्धारित भएको छैन भारत, चीन र अरु देशसंगको वैदेशिक वाणिज्य सम्बन्ध कुनै मजबुत सन्धिको जगमा स्थापित भएको छैन\n",
            " PREDICTED: सरकारको स्पष्ट वाणिज्य नीति पनि निर्धारित भएको छैन भारत , चीन र अरु वैदेशिक वाणिज्य सम्बन्ध कुनै मजबुत सन्धिको स्थापित भएको छैन\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: But the trade channedl that have exited - within the country and through a few high passes into the Tibet region of China, and across the flat 500 mile Indian frontier - are largely north to south channels.\n",
            "    TARGET: यिनै कारणले गर्दा र उत्तर दक्षिणको सम्पर्क पनि सजिलो नहुंदा देशभित्रको व्यापारमा मात्र होइन देशबाहिरको व्यापारमा पनि बाधा परेको छ\n",
            " PREDICTED: यिनै कारणले गर्दा र उत्तर दक्षिणको सम्पर्क पनि सजिलो व्यापारमा मात्र होइन व्यापारमा पनि बाधा परेको छ\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Three Year Plan Progress\n",
            "    TARGET: त्रिवर्षीय योजनाको प्रगति\n",
            " PREDICTED: त्रिवर्षीय योजनाको प्रगति\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Table 1 gives the financial allocations and actual expenditures during the Second Plan.\n",
            "    TARGET: उपर्युक्त तालिकाअनुसार त्रिवर्षीय योजनामा लक्ष्यअनुसार छुट याइएको कुल रकमको झण्डै शत प्रतिशत   खर्च हुन आएको छ\n",
            " PREDICTED: उपर्युक्त तालिकाअनुसार त्रिवर्षीय योजनामा लक्ष्यअनुसार छुट याइएको कुल रकमको झण्डै शत प्रतिशत खर्च हुन आएको छ\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "def translate(sentence: str):\n",
        "    # Define the device, tokenizers, and model\n",
        "    device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "    config = get_config()\n",
        "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
        "    tokenizer_tgt = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_tgt']))))\n",
        "    model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
        "\n",
        "    # Load the pretrained weights\n",
        "    # model_filename = latest_weights_file_path(config)\n",
        "    state = torch.load('/media/bikasherl/New Volume/Bajra_Tech/Week 9/tmodel_559.pt')\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "    # if the sentence is a number use it as an index to the test set\n",
        "    label = \"\"\n",
        "    if type(sentence) == int or sentence.isdigit():\n",
        "        id = int(sentence)\n",
        "        # ds = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='all')\n",
        "        ds = load_dataset(f\"{config['datasource']}\")\n",
        "        ds = BilingualDataset(ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "        sentence = ds[id]['src_text']\n",
        "        label = ds[id][\"tgt_text\"]\n",
        "    seq_len = config['seq_len']\n",
        "\n",
        "    # translate the sentence\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Precompute the encoder output and reuse it for every generation step\n",
        "        source = tokenizer_src.encode(sentence)\n",
        "        source = torch.cat([\n",
        "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64), \n",
        "            torch.tensor(source.ids, dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
        "        ], dim=0).to(device)\n",
        "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
        "        encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "        # Initialize the decoder input with the sos token\n",
        "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
        "\n",
        "        # Print the source sentence and target start prompt\n",
        "        if label != \"\": print(f\"{f'ID: ':>12}{id}\") \n",
        "        print(f\"{f'SOURCE: ':>12}{sentence}\")\n",
        "        if label != \"\": print(f\"{f'TARGET: ':>12}{label}\") \n",
        "        print(f\"{f'PREDICTED: ':>12}\", end='')\n",
        "\n",
        "        # Generate the translation word by word\n",
        "        while decoder_input.size(1) < seq_len:\n",
        "            # build mask for target and calculate output\n",
        "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n",
        "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "            # project next token\n",
        "            prob = model.project(out[:, -1])\n",
        "            _, next_word = torch.max(prob, dim=1)\n",
        "            decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "            # print the translated word\n",
        "            print(f\"{tokenizer_tgt.decode([next_word.item()])}\", end=' ')\n",
        "\n",
        "            # break if we predict the end of sentence token\n",
        "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
        "                break\n",
        "\n",
        "    # convert ids to tokens\n",
        "    return tokenizer_tgt.decode(decoder_input[0].tolist())\n",
        "    \n",
        "#read sentence from argument\n",
        "# translate(sys.argv[1] if len(sys.argv) > 1 else \"I am not a very good a student.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "    SOURCE: The planned outlay in the public sector was virtually achieved.\n",
            " PREDICTED: तालिका नं . १ ( क ) मा विभिन्न क्षेत्रको विकासका लागि योजनामा छुट याइएको रकम छुट याइएको साथ साथ साथ । । ।  "
          ]
        }
      ],
      "source": [
        "t=translate(\"The planned outlay in the public sector was virtually achieved.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
